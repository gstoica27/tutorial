{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ray.experimental.tf_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d3cd5c729a57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ray.experimental.tf_utils'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import ray\n",
    "import ray.experimental.tf_utils\n",
    "\n",
    "ray.init()\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "NUM_BATCHES = 1\n",
    "NUM_ITERS = 201\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, x, y):\n",
    "        # Seed TensorFlow to make the script deterministic.\n",
    "        tf.set_random_seed(0)\n",
    "        # Define the inputs.\n",
    "        self.x_data = tf.constant(x, dtype=tf.float32)\n",
    "        self.y_data = tf.constant(y, dtype=tf.float32)\n",
    "        # Define the weights and computation.\n",
    "        w = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "        b = tf.Variable(tf.zeros([1]))\n",
    "        y = w * self.x_data + b\n",
    "        # Define the loss.\n",
    "        self.loss = tf.reduce_mean(tf.square(y - self.y_data))\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "        self.grads = optimizer.compute_gradients(self.loss)\n",
    "        self.train = optimizer.apply_gradients(self.grads)\n",
    "        # Define the weight initializer and session.\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess = tf.Session()\n",
    "        # Additional code for setting and getting the weights\n",
    "        self.variables = ray.experimental.tf_utils.TensorFlowVariables(self.loss, self.sess)\n",
    "        # Return all of the data needed to use the network.\n",
    "        self.sess.run(init)\n",
    "\n",
    "    # Define a remote function that trains the network for one step and returns the\n",
    "    # new weights.\n",
    "    def step(self, weights):\n",
    "        # Set the weights in the network.\n",
    "        self.variables.set_weights(weights)\n",
    "        # Do one step of training.\n",
    "        self.sess.run(self.train)\n",
    "        # Return the new weights.\n",
    "        return self.variables.get_weights()\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.variables.get_weights()\n",
    "\n",
    "# Define a remote function for generating fake data.\n",
    "@ray.remote(num_return_vals=2)\n",
    "def generate_fake_x_y_data(num_data, seed=0):\n",
    "    # Seed numpy to make the script deterministic.\n",
    "    np.random.seed(seed)\n",
    "    x = np.random.rand(num_data)\n",
    "    y = x * 0.1 + 0.3\n",
    "    return x, y\n",
    "\n",
    "# Generate some training data.\n",
    "batch_ids = [generate_fake_x_y_data.remote(BATCH_SIZE, seed=i) for i in range(NUM_BATCHES)]\n",
    "x_ids = [x_id for x_id, y_id in batch_ids]\n",
    "y_ids = [y_id for x_id, y_id in batch_ids]\n",
    "# Generate some test data.\n",
    "x_test, y_test = ray.get(generate_fake_x_y_data.remote(BATCH_SIZE, seed=NUM_BATCHES))\n",
    "\n",
    "# Create actors to store the networks.\n",
    "remote_network = ray.remote(Network)\n",
    "actor_list = [remote_network.remote(x_ids[i], y_ids[i]) for i in range(NUM_BATCHES)]\n",
    "\n",
    "# Get initial weights of some actor.\n",
    "weights = ray.get(actor_list[0].get_weights.remote())\n",
    "\n",
    "# Do some steps of training.\n",
    "for iteration in range(NUM_ITERS):\n",
    "    # Put the weights in the object store. This is optional. We could instead pass\n",
    "    # the variable weights directly into step.remote, in which case it would be\n",
    "    # placed in the object store under the hood. However, in that case multiple\n",
    "    # copies of the weights would be put in the object store, so this approach is\n",
    "    # more efficient.\n",
    "    weights_id = ray.put(weights)\n",
    "    # Call the remote function multiple times in parallel.\n",
    "    new_weights_ids = [actor.step.remote(weights_id) for actor in actor_list]\n",
    "    # Get all of the weights.\n",
    "    new_weights_list = ray.get(new_weights_ids)\n",
    "    # Add up all the different weights. Each element of new_weights_list is a dict\n",
    "    # of weights, and we want to add up these dicts component wise using the keys\n",
    "    # of the first dict.\n",
    "    weights = {variable: sum(weight_dict[variable] for weight_dict in new_weights_list) / NUM_BATCHES for variable in new_weights_list[0]}\n",
    "    # Print the current weights. They should converge to roughly to the values 0.1\n",
    "    # and 0.3 used in generate_fake_x_y_data.\n",
    "    if iteration % 20 == 0:\n",
    "        print(\"Iteration {}: weights are {}\".format(iteration, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
